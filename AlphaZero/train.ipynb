{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from mcts import MCT, Node\n",
    "from net import PolicyValueNet\n",
    "from src_net import init_state\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHESSBOARD_SIZE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    for i in range(len(data)):\n",
    "        actions, probs = data[i][1]\n",
    "        action_prob = np.zeros((CHESSBOARD_SIZE, CHESSBOARD_SIZE), dtype=float)\n",
    "        for j in range(len(actions)):\n",
    "            action_prob[actions[j]] = probs[j]\n",
    "        data[i][1] = action_prob\n",
    "\n",
    "    augment_data = []\n",
    "    for state, action_prob, v in data:\n",
    "        for i in [1, 2, 3, 4]:\n",
    "            equal_state = np.rot90(state, i, axes=(-2,-1))\n",
    "            equal_action_prob = np.rot90(action_prob, i, axes=(-2, -1))\n",
    "\n",
    "            augment_data.append([equal_state, equal_action_prob, v])\n",
    "            augment_data.append([np.flip(equal_state, axis=(-1)), np.flip(equal_action_prob, axis=(-1)), v])        \n",
    "    return augment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = torch.tensor(np.array([d[0] for d in data])).float()\n",
    "        self.y = [(torch.tensor(np.array(d[1])).float(), torch.tensor(np.array(d[2])).float()) for d in data]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "\n",
    "\n",
    "def train_epoch(net, train_iter, loss, updater):\n",
    "    net.train()\n",
    "\n",
    "    metric = d2l.Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        y_pred = net(X)\n",
    "        policy_loss, value_loss = loss(y_pred, y)\n",
    "        l = policy_loss+value_loss\n",
    "        updater.zero_grad()\n",
    "        l.mean().backward()\n",
    "        updater.step()\n",
    "        metric.add(float(policy_loss.sum()), float(value_loss.sum()), y_pred[0].shape[0])\n",
    "\n",
    "    return metric[0]/metric[2], metric[1]/metric[2]\n",
    "\n",
    "\n",
    "def train(net, loss, updater, data, batch_size, num_epochs, sample_size):\n",
    "    train_dataset = MyDataset(random.sample(data, sample_size))\n",
    "    train_iter = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        policy_loss, value_loss = train_epoch(net, train_iter, loss, updater)\n",
    "      \n",
    "    return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.policy_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.value_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, y_pred, y):\n",
    "        batch_size = y_pred[0].shape[0]\n",
    "        policy_loss = self.policy_loss(y_pred[0].reshape(batch_size, -1), y[0].reshape(batch_size, -1))\n",
    "        value_loss = self.value_loss(y_pred[1], y[1])\n",
    "        return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 32*8*300\n",
    "data_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "batch_size = 1024\n",
    "num_epochs = 5\n",
    "num_games = 1\n",
    "time_step = 5*60\n",
    "def sample_size():\n",
    "    return batch_size\n",
    "\n",
    "lr = 2e-3\n",
    "params = 'reversi'+str(CHESSBOARD_SIZE)+'.params'\n",
    "\n",
    "net = PolicyValueNet(CHESSBOARD_SIZE)\n",
    "# net.load_state_dict(torch.load(params))\n",
    "policy_value_loss = PolicyValueLoss()\n",
    "\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4 cost_time: 123.63841080665588 loss: 4.372487366199493 entropy_loss: 3.57893967628479\n",
      "step: 5 cost_time: 129.00777959823608 loss: 4.534661054611206 entropy_loss: 3.5765976905822754\n",
      "step: 6 cost_time: 122.49259352684021 loss: 4.387017548084259 entropy_loss: 3.572873115539551\n",
      "step: 7 cost_time: 127.82873725891113 loss: 4.188644826412201 entropy_loss: 3.571941614151001\n",
      "step: 8 cost_time: 122.00814485549927 loss: 4.096157252788544 entropy_loss: 3.566882848739624\n",
      "step: 9 cost_time: 125.86330652236938 loss: 4.04937943816185 entropy_loss: 3.561460494995117\n",
      "step: 10 cost_time: 125.47502589225769 loss: 4.0442891120910645 entropy_loss: 3.558140993118286\n",
      "step: 11 cost_time: 110.25218081474304 loss: 3.99556365609169 entropy_loss: 3.550175189971924\n",
      "step: 12 cost_time: 116.82256388664246 loss: 4.065080165863037 entropy_loss: 3.549067497253418\n",
      "step: 13 cost_time: 119.94454288482666 loss: 4.071060597896576 entropy_loss: 3.5499091148376465\n",
      "step: 14 cost_time: 116.74259614944458 loss: 4.108671545982361 entropy_loss: 3.5376522541046143\n",
      "step: 15 cost_time: 115.32984495162964 loss: 4.014047026634216 entropy_loss: 3.5372238159179688\n",
      "step: 16 cost_time: 111.10854125022888 loss: 3.9631959795951843 entropy_loss: 3.5208683013916016\n",
      "step: 17 cost_time: 121.70082688331604 loss: 3.9540378153324127 entropy_loss: 3.513389825820923\n",
      "step: 18 cost_time: 111.32058238983154 loss: 3.848919987678528 entropy_loss: 3.496966600418091\n",
      "step: 19 cost_time: 103.0908133983612 loss: 3.919091761112213 entropy_loss: 3.5005712509155273\n",
      "step: 20 cost_time: 104.62852144241333 loss: 3.855485826730728 entropy_loss: 3.490952968597412\n",
      "step: 21 cost_time: 114.35885524749756 loss: 3.805559515953064 entropy_loss: 3.4780797958374023\n",
      "step: 22 cost_time: 108.42540001869202 loss: 3.8284259736537933 entropy_loss: 3.4695699214935303\n",
      "step: 23 cost_time: 109.90703845024109 loss: 3.7463541328907013 entropy_loss: 3.4605565071105957\n",
      "step: 24 cost_time: 119.14701843261719 loss: 3.7140744030475616 entropy_loss: 3.442595958709717\n",
      "step: 25 cost_time: 109.0928726196289 loss: 3.8219298720359802 entropy_loss: 3.4446349143981934\n",
      "step: 26 cost_time: 106.21151733398438 loss: 3.7094916999340057 entropy_loss: 3.4253671169281006\n",
      "step: 27 cost_time: 92.34216046333313 loss: 3.691027194261551 entropy_loss: 3.399822235107422\n",
      "step: 28 cost_time: 99.17600774765015 loss: 3.6982454359531403 entropy_loss: 3.408221483230591\n",
      "step: 29 cost_time: 98.96209597587585 loss: 3.641366958618164 entropy_loss: 3.381826400756836\n",
      "step: 30 cost_time: 89.77552914619446 loss: 3.708863705396652 entropy_loss: 3.404205083847046\n",
      "step: 31 cost_time: 102.72336435317993 loss: 3.6026725322008133 entropy_loss: 3.360363006591797\n",
      "step: 32 cost_time: 92.42963194847107 loss: 3.5825369507074356 entropy_loss: 3.351090669631958\n",
      "step: 33 cost_time: 100.37227058410645 loss: 3.565730005502701 entropy_loss: 3.3575572967529297\n",
      "step: 34 cost_time: 106.80183172225952 loss: 3.6275306940078735 entropy_loss: 3.349396228790283\n",
      "step: 35 cost_time: 103.9479329586029 loss: 3.5845066756010056 entropy_loss: 3.33488130569458\n",
      "step: 36 cost_time: 114.76610040664673 loss: 3.589399427175522 entropy_loss: 3.335252523422241\n",
      "step: 37 cost_time: 104.97617506980896 loss: 3.527408868074417 entropy_loss: 3.3019378185272217\n",
      "step: 38 cost_time: 99.80805206298828 loss: 3.5054301023483276 entropy_loss: 3.2898168563842773\n",
      "step: 39 cost_time: 98.2385094165802 loss: 3.499772220849991 entropy_loss: 3.2834763526916504\n",
      "step: 40 cost_time: 103.97369694709778 loss: 3.463820159435272 entropy_loss: 3.260380744934082\n",
      "step: 41 cost_time: 104.83261203765869 loss: 3.4335642904043198 entropy_loss: 3.235171318054199\n",
      "step: 42 cost_time: 98.86940574645996 loss: 3.4117965400218964 entropy_loss: 3.2291531562805176\n",
      "step: 43 cost_time: 99.09263038635254 loss: 3.4923062324523926 entropy_loss: 3.2543811798095703\n",
      "step: 44 cost_time: 104.15417790412903 loss: 3.511086016893387 entropy_loss: 3.235928773880005\n",
      "step: 45 cost_time: 103.03478360176086 loss: 3.441735327243805 entropy_loss: 3.225055456161499\n",
      "step: 46 cost_time: 109.80041289329529 loss: 3.41921728849411 entropy_loss: 3.202524423599243\n",
      "step: 47 cost_time: 97.56516242027283 loss: 3.446709856390953 entropy_loss: 3.2254464626312256\n",
      "step: 48 cost_time: 106.16896605491638 loss: 3.3837767988443375 entropy_loss: 3.1819143295288086\n",
      "step: 49 cost_time: 93.46622276306152 loss: 3.373814284801483 entropy_loss: 3.174225330352783\n",
      "step: 50 cost_time: 94.04440975189209 loss: 3.3778238594532013 entropy_loss: 3.1419384479522705\n",
      "step: 51 cost_time: 99.6891565322876 loss: 3.344080090522766 entropy_loss: 3.1345643997192383\n",
      "step: 52 cost_time: 104.99151730537415 loss: 3.394199013710022 entropy_loss: 3.1608774662017822\n",
      "step: 53 cost_time: 106.95704674720764 loss: 3.3523170799016953 entropy_loss: 3.1315295696258545\n",
      "step: 54 cost_time: 107.6400032043457 loss: 3.3863430321216583 entropy_loss: 3.1368253231048584\n",
      "step: 55 cost_time: 98.16608595848083 loss: 3.3629744946956635 entropy_loss: 3.1284425258636475\n",
      "step: 56 cost_time: 102.35710430145264 loss: 3.363511562347412 entropy_loss: 3.120065212249756\n",
      "step: 57 cost_time: 94.20890545845032 loss: 3.317963942885399 entropy_loss: 3.0889534950256348\n",
      "step: 58 cost_time: 110.88857555389404 loss: 3.3556502014398575 entropy_loss: 3.1320276260375977\n",
      "step: 59 cost_time: 119.13533210754395 loss: 3.3219353705644608 entropy_loss: 3.1150007247924805\n",
      "step: 60 cost_time: 116.77914094924927 loss: 3.341681808233261 entropy_loss: 3.0853517055511475\n",
      "step: 61 cost_time: 99.71462750434875 loss: 3.3133849799633026 entropy_loss: 3.0858378410339355\n",
      "step: 62 cost_time: 94.3210666179657 loss: 3.3773596584796906 entropy_loss: 3.101290225982666\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    cnt += 1\n",
    "    mct = MCT(policy_value_fn=net)\n",
    "\n",
    "    start = time.time()\n",
    "    game_cnt = 0\n",
    "    while(game_cnt<num_games):\n",
    "        mct.set_root(Node(state_now=init_state(CHESSBOARD_SIZE)))\n",
    "        data_buffer.extend(preprocess_data(mct.self_play(num=400)))\n",
    "        game_cnt += 1\n",
    "        \n",
    "\n",
    "    updater = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    # updater = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    if(len(data_buffer)<sample_size()):\n",
    "        continue\n",
    "    policy_loss, value_loss = train(net, policy_value_loss, updater, data_buffer, batch_size, num_epochs, \n",
    "                                        sample_size=sample_size())\n",
    "\n",
    "    print('step:', cnt, 'cost_time:', time.time()-start, \n",
    "                            'loss:', policy_loss+value_loss, 'entropy_loss:', policy_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'reversi'+str(CHESSBOARD_SIZE)+'.params')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS321",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "798e7a250d91faa5167978c29dc4d749c5ae494daaa4d2a87c2da3edb2bb892e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
